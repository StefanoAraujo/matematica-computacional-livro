
==== O método de Gauss-Seidel

Este método particulariza o método iterativo (II), por considerar a matriz latexmath:[$Q$] como a parte triangular inferior da matriz latexmath:[$A$], incluindo os elementos da diagonal de latexmath:[$A$], isto é
[latexmath]
++++
\[
Q=\begin{bmatrix} 
a_{11} & 0 & \ldots & 0 \\
a_{21} & a_{22} & \ldots & 0\\
\vdots& \vdots&  & \vdots\\
a_{n1} & a_{n2} & \ldots & a_{nn} 
\end{bmatrix} 
\]
++++
então
[latexmath]
++++
\[
(I-Q^{-1}A)= -Q^{-1}(A-Q)= -Q^{-1}  
\begin{bmatrix} 
0 &a_{12} & a_{13} & \ldots & a_{1n} \\
0 &  0    & a_{23} & \ldots & a_{2n}\\
\vdots & \vdots& \vdots&  & \vdots\\
0 & 0 & 0 & \ldots & a_{(n-1)n}\\
0 & 0 & 0 & \ldots & 0 
\end{bmatrix}
\]
++++
Portanto, o processo iterativo de Gauss-Seidel é:
[latexmath]
++++
\[
\begin{bmatrix} 
x^{(k)}_1\\
x^{(k)}_2\\
\vdots\\
x^{(k)}_n
\end{bmatrix}
= Q^{-1}\begin{bmatrix} 
b_1 \\
b_2 \\
\vdots\\
b_n
\end{bmatrix} - Q^{-1}
\begin{bmatrix} 
0 &a_{12} & a_{13} & \ldots & a_{1n} \\
0 &  0    & a_{23} & \ldots & a_{2n}\\
\vdots & \vdots& \vdots&  & \vdots\\
0 & 0 & 0 & \ldots & a_{(n-1)n}\\
0 & 0 & 0 & \ldots & 0 
\end{bmatrix}
\begin{bmatrix} 
x^{(k-1)}_1\\
x^{(k-1)}_2\\
\vdots\\
x^{(k-1)}_n
\end{bmatrix} \qquad k\geq 1
\]
++++


Teorema 5.4:: Se latexmath:[$A$] é estritamente diagonalmente dominante, então a sequencia gerada pela iteração de Gauss-Seidel converge para a solução de latexmath:[$Ax=b$] para qualquer vetor inicial. 

O seguinte algoritmo aplica o processo iterativo de Gauss-Seidel:

Algoritmo 5.8::

Dados Iniciais::: latexmath:[$x^{(0)}=\begin{bmatrix}
x^{(0)}_1\\
x^{(0)}_2\\
\vdots\\
x^{(0)}_n
\end{bmatrix}$], latexmath:[$M=$] quantidade de iterações.


//latexmath:[$u\leftarrow x^{(0)}$]


Para latexmath:[$k=1,2,\ldots, M$] faça 

latexmath:[$\qquad$ Para $i=1,2,\ldots, n$] faça

[latexmath]
++++
\[
u_i \leftarrow \dfrac{1}{a_{ii}} \left(b_i -\sum\limits^{n}_{\stackrel{j=1}{j\neq i}} a_{ij} x_j^{(k-1)} \right)
\]
++++

latexmath:[$\qquad$ Para $i=1,2,\ldots, n$] faça

[latexmath]
++++
\[
x^{(k)}_i \leftarrow u_i
\]
++++


Saída:: 
[latexmath]
++++
\[
x^{(M)}=\begin{bmatrix}
x^{(M)}_1\\
x^{(M)}_2\\
\vdots\\
x^{(M)}_n
\end{bmatrix}
\]
++++



[NOTE] 
No método de Jacobi-Richardson, as novas componentes do vetor latexmath:[$x$] podem ser calculadas de forma simultânea, enquanto no Método de Gauss-Seidel as componentes devem ser calculadas de forma serial, devido a que o calculo da nova componente latexmath:[$x_i$] precisa dos novos valores de latexmath:[$x_1,x_2,\ldots,x_{i-1}$].

 


